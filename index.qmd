---
title: "K means clustering- Data Science Capstone"
author: "Bala Venkataprasad Sadhana Vittal"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "Kmeans"?

This is an introduction to Kernel regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].

### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

Cite: Bradley, P. S., Bennett, K. P., & Demiriz, A. (2000). Constrained
k-means clustering. Microsoft Research, Redmond, 20(0), 0.

Summary: K-Means clustering algorithm maintains that each cluster
contains a minimum number of points. This algorithm helps when
constraints during the cluster assignment step, which can be solved
through linear programming or network simplex methods. The main
objective is to guarantee a sufficient population within each cluster.
Constrained K-Means are less prone to local minima than traditional
K-Means, suggesting the benefits of adding constraints even when
allowing empty clusters.

this paper introduces the concept of robust clustering by adding an
"outlier" cluster with a fixed distance to gather outliers far from true
clusters. Constraints are also discussed for forcing selected data
points into the same cluster, enabling the incorporation of domain
knowledge, or enforcing consistency across successive cluster solutions
on related data.

we can conclude that the extension's ability to ensure a minimum
population in each cluster and the advantage of Constrained K-Means in
avoiding local minima. The text also presents results, including
objective function ratios, indicating that the Constrained K-Means
solution is often superior when appropriate constraints and cluster
sizes are chosen. Overall, the extension of K-Means with constraints
opens up possibilities for robust and domain-specific clustering.

paper 2: cite: Application of k-Means Clustering algorithm for
prediction of Studentsâ€™ Academic Performance Oyelade, O. J Oladipupo, O.
O ,Obagbuwa, I. C

The paper discusses the development and implementation of a system for
analyzing students' academic performance using cluster analysis and
standard statistical algorithms. The focus is on using the k-means
clustering algorithm to group students based on their scores, providing
a tool for monitoring academic progression in higher institutions.

The introduction emphasizes the importance of monitoring students'
academic performance, with Graded Point Average (GPA) being a common
indicator. The paper proposes the use of k-means clustering to
categorize students into different performance levels, enabling a more
comprehensive view than traditional grouping based on average scores.

The methodology section details the development of the k-means
clustering algorithm, explaining the steps involved and highlighting its
ease of implementation, scalability, and adaptability to sparse data.
The paper then presents results from applying the model to a dataset
from a Nigerian university, showing cluster sizes and overall
performances for different cluster numbers. The results section includes
tables and graphs illustrating the performance analysis based on cluster
size. The paper discusses the trends observed in students' performance
within various clusters, providing insights into the effectiveness of
the k-means clustering approach.

In conclusion, the paper emphasizes the simplicity and efficiency of the
k-means clustering algorithm as a tool for monitoring students' academic
performance in higher institutions. It highlights the potential benefits
of using data mining methods, such as clustering algorithms, to discover
key characteristics from students' performance data for future
predictions. Overall, the paper contributes to the discussion on
leveraging clustering techniques for academic performance analysis and
decision-making in educational institutions.

Week 3:

Paper 1:

Cite: S. Na, L. Xumin and G. Yong, "Research on k-means Clustering
Algorithm: An Improved k-means Clustering Algorithm," 2010 Third
International Symposium on Intelligent Information Technology and
Security Informatics, Jian, China, 2010, pp. 63-67, doi:
10.1109/IITSI.2010.74.

Summary:

The article introduces clustering as a method for classification of raw
data and discovering hidden patterns in datasets. It says the
application of clustering techniques in various fields such as
artificial intelligence, biology, customer relationship management, data
mining, and more. The paper focus on the k-means clustering algorithm, a
numerical, unsupervised, and iterative method known for its simplicity
and speed. The standard k-means algorithm involves selection of initial
cluster centers randomly and iteratively assigning data objects to the
nearest center until convergence.

This article focus on the outcomes of the standard k-means algorithm,
specifically it calculates the distances for each data object in every
iteration, leading to inefficiencies in large datasets. To address these
issues, the paper presents an improved k-means clustering algorithm. The
key idea involves around using two data structures to retain cluster
labels and distances, reducing the need for repeated distance
calculations. This improved algorithm aims to enhance the speed and
efficiency of clustering while maintaining accuracy.

The paper explains the steps of the improved algorithm, definines its
time complexity, which is claimed to be better than the standard k-means
algorithm. Experimental results using UCI datasets demonstrate the
efficiency of the proposed algorithm in terms of reduced running time
and improved accuracy. Two experiments are conducted, one with iris and
glass datasets and another with the letter dataset on different values
of k. The results indicate that the improved k-means algorithm
outperforms the standard k-means algorithm in terms of overall execution
time, especially for large-capacity databases.

In conclusion, the paper introduces an improved k-means clustering
algorithm that addresses the shortcomings of the standard k-means
algorithm. Experimental results suggest that the proposed algorithm is
both faster and more accurate, making it a feasible and advantageous
alternative for clustering tasks.

Paper 2:

Cite: Kodinariya, T. M., & Makwana, P. R. (2013). Review on determining
number of Cluster in K-Means Clustering. International Journal, 1(6),
90-95.

Summary: The article says about different approaches to select the right
number of clusters in K-Means clustering. These approaches include the
rule of thumb, the elbow method, the information criterion approach, an
information theoretic approach, choosing k using the silhouette, and
cross-validation. Each approach is explained with its advantages and
limitations.

The elbow method involves visually identifying a point where the cost
function (distortion) drops dramatically, suggesting the optimal number
of clusters. The information criterion approach uses criteria like AIC
and BIC to balance the likelihood increase with additional parameters.
An information theoretic approach, based on rate-distortion theory,
introduces the jump statistic to identify the right number of clusters.
The silhouette method evaluates within-cluster tightness and separation
to determine the optimal number of clusters.

Finally, the article concludes cross-validation as an approach for
estimating the number of clusters based on stability of cluster.
Different studies and modifications of cross-validation are presented,
focusing on their effectiveness and robustness on various datasets.

we can conclude, the article clarifies the significance of studying the
properties of K-Means clustering, especially in determining the right
number of clusters. It acknowledges the controdiction surrounding this
issue and explores approaches that may provide meaningful insights into
the clustering process, addressing the practical challenges in various
application areas.
